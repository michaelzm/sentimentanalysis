{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "phantom-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "import zipfile\n",
    "import pickle\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from elmo_dataset_joiner import DatasetWorker, VocabularyWorker\n",
    "from elmo_performance import PerformanceViewer\n",
    "from model_factory import ModelFactory\n",
    "\n",
    "np.random.seed(1234567890)\n",
    "tf.random.set_seed(1234567890)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suspected-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetSeeds():\n",
    "    np.random.seed(1234567890)\n",
    "    tf.random.set_seed(1234567890)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "objective-disco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "#tensorflow stuff\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "scenic-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow stuff in case mem overflows or something like that\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nutritional-departure",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenize dataset: 3101it [00:00, 387745.55it/s]\n",
      "split dataset labels: 3101it [00:00, 33354.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start building model for  sentiments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting tokens & labels to ids : 100%|█████████████████████████████████████████| 8698/8698 [00:30<00:00, 283.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating model for sentiments\n",
      "Epoch 1/5\n",
      "326/326 [==============================] - 75s 230ms/step - crf_loss: 4.5431 - accuracy: 0.9845 - val_crf_loss_val: 1.9165 - val_val_accuracy: 0.9874\n",
      "Epoch 2/5\n",
      "326/326 [==============================] - 73s 224ms/step - crf_loss: 3.1162 - accuracy: 0.9888 - val_crf_loss_val: 1.7634 - val_val_accuracy: 0.9877\n",
      "Epoch 3/5\n",
      "326/326 [==============================] - 73s 224ms/step - crf_loss: 2.7972 - accuracy: 0.9893 - val_crf_loss_val: 1.7153 - val_val_accuracy: 0.9872\n",
      "Epoch 4/5\n",
      "326/326 [==============================] - 73s 224ms/step - crf_loss: 2.5243 - accuracy: 0.9895 - val_crf_loss_val: 1.7718 - val_val_accuracy: 0.9880\n",
      "Epoch 5/5\n",
      "326/326 [==============================] - 73s 224ms/step - crf_loss: 2.3138 - accuracy: 0.9900 - val_crf_loss_val: 1.7253 - val_val_accuracy: 0.9879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenize dataset: 3101it [00:00, 281960.08it/s]\n",
      "split dataset labels: 3101it [00:00, 34088.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision 0.752\n",
      "Recall 0.515\n",
      "F1-measure 0.612\n",
      "start building model for  aspects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting tokens & labels to ids : 100%|█████████████████████████████████████████| 8764/8764 [00:29<00:00, 296.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating model for aspects\n",
      "Epoch 1/4\n",
      "329/329 [==============================] - 77s 235ms/step - crf_loss: 4.7808 - accuracy: 0.9841 - val_crf_loss_val: 0.5044 - val_val_accuracy: 0.9881\n",
      "Epoch 2/4\n",
      "329/329 [==============================] - 76s 232ms/step - crf_loss: 3.4579 - accuracy: 0.9882 - val_crf_loss_val: 0.9092 - val_val_accuracy: 0.9889\n",
      "Epoch 3/4\n",
      "329/329 [==============================] - 76s 232ms/step - crf_loss: 3.0839 - accuracy: 0.9889 - val_crf_loss_val: 0.5085 - val_val_accuracy: 0.9888\n",
      "Epoch 4/4\n",
      "329/329 [==============================] - 76s 232ms/step - crf_loss: 2.7754 - accuracy: 0.9895 - val_crf_loss_val: 0.7512 - val_val_accuracy: 0.9884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenize dataset: 3101it [00:00, 387676.21it/s]\n",
      "split dataset labels: 3101it [00:00, 33717.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision 0.677\n",
      "Recall 0.582\n",
      "F1-measure 0.626\n",
      "start building model for  modifiers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting tokens & labels to ids : 100%|█████████████████████████████████████████| 8830/8830 [00:29<00:00, 296.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating model for modifiers\n",
      "Epoch 1/5\n",
      "332/332 [==============================] - 75s 226ms/step - crf_loss: 2.4435 - accuracy: 0.9921 - val_crf_loss_val: 0.4910 - val_val_accuracy: 0.9958\n",
      "Epoch 2/5\n",
      "332/332 [==============================] - 74s 224ms/step - crf_loss: 1.3975 - accuracy: 0.9956 - val_crf_loss_val: 0.3517 - val_val_accuracy: 0.9959\n",
      "Epoch 3/5\n",
      "332/332 [==============================] - 74s 224ms/step - crf_loss: 1.2769 - accuracy: 0.9958 - val_crf_loss_val: 0.2410 - val_val_accuracy: 0.9955\n",
      "Epoch 4/5\n",
      "332/332 [==============================] - 75s 224ms/step - crf_loss: 1.1691 - accuracy: 0.9959 - val_crf_loss_val: 0.1894 - val_val_accuracy: 0.9960\n",
      "Epoch 5/5\n",
      "332/332 [==============================] - 74s 224ms/step - crf_loss: 1.0765 - accuracy: 0.9961 - val_crf_loss_val: 0.3442 - val_val_accuracy: 0.9960\n",
      "\n",
      "Precision 0.817\n",
      "Recall 0.508\n",
      "F1-measure 0.627\n"
     ]
    }
   ],
   "source": [
    "## first step is to create the different datasets for sentiments, aspects and modifiers\n",
    "extracts = [\"sentiments\", \"aspects\", \"modifiers\"]\n",
    "\n",
    "vw= VocabularyWorker()\n",
    "\n",
    "#data_laptop contains all reviews inclusive the test data\n",
    "filename = r'data_laptop_absa.json'\n",
    "\n",
    "#Embedding list from pickle file\n",
    "embedding = pickle.load( open(\"elmo_embedding.pkl\", \"rb\" ) )\n",
    "\n",
    "##test_id_list contains all the test data which we keep seperate from the other data\n",
    "test_id_list = pickle.load(open(\"test_id_list.pkl\", \"rb\"))\n",
    "\n",
    "with open(filename,'r', encoding='utf8') as infile:\n",
    "    review_data = json.load(infile)\n",
    "\n",
    "#confusion matrix will get stored inside here\n",
    "# idx 0 -> sentiments, idx 1 -> aspects, idx 2 -> modifiers\n",
    "results = []\n",
    "filename = r'data_laptop_absa.json'\n",
    "\n",
    "#gridsearch evaluated 5 epochs for sentiments and modifiers and 4 for aspects\n",
    "epochs_parameter = {\"sentiments\":5, \"modifiers\":5, \"aspects\":4}\n",
    "\n",
    "for extract in extracts:\n",
    "    extraction_of = extract\n",
    "    print(\"start building model for \", extraction_of)\n",
    "    \n",
    "    max_seq_length = 100\n",
    "    ds = DatasetWorker(review_data)\n",
    "    \n",
    "    #tokenize data\n",
    "    ds.applyPreprocessing()\n",
    "    #todo set extraction_of correct\n",
    "    ds.setExtractionOf(extraction_of)\n",
    "    \n",
    "    #options for splitDataset = every_review, every_review_without_uncertain\n",
    "    #what we get is for example \n",
    "    #tokens[0]  [['computer', 'works', 'great', '.'], 1, 'train']\n",
    "    #labels[0] ['O', 'O', 'O', 'O']\n",
    "    tokens, labels =ds.splitDataset(\"every_review_without_uncertain\", test_id_list)\n",
    "    \n",
    "\n",
    "    #build vocab and add embedding#\n",
    "    #with elmo, every token of each sentence gets its individual elmo vecotr\n",
    "\n",
    "    # we have a total of 54352 vectors\n",
    "    vocab_size = len (embedding)\n",
    "\n",
    "    #each vector has 1024 dimensions (smallest elmo dataset)\n",
    "    embed_size = 1024\n",
    "\n",
    "    embedding_vectors = np.zeros((vocab_size, embed_size))\n",
    "\n",
    "    #embedding-vectors for Embedding layer \n",
    "    for i,embedding_element in enumerate(embedding):\n",
    "        #we only extract the vector data\n",
    "        token_vector = embedding_element[1]\n",
    "        embedding_vectors[i] = token_vector\n",
    "        \n",
    "    #extract labelclasses\n",
    "    all_labelclasses = set()\n",
    "    for row in labels:\n",
    "        all_labelclasses.update(row)\n",
    "    all_labelclasses=list(all_labelclasses)\n",
    "    all_labelclasses.sort()\n",
    "\n",
    "    labelclass_to_id = dict(zip(all_labelclasses,list(range(len(all_labelclasses)))))\n",
    "  \n",
    "    \n",
    "    #define number of labelclasses as n_tags\n",
    "    n_tags = len(list(labelclass_to_id.keys()))\n",
    "\n",
    "    #max sequence length defines the length each review should get padded to\n",
    "    max_seq_length = 100\n",
    "\n",
    "    #convert token and label to ids according to the embedding and split into train and test\n",
    "    #test_tokens is needed for calculation of the confusion matrix\n",
    "    #x_train,x_test, y_train, y_test are for training and testing respectively\n",
    "    # train_tokens and test_tokens are used for better analyzing the data because they dont contain padded elements\n",
    "    #because y data changes per extraction_of we have to let this run through every time\n",
    "    x_train, x_test, y_train, y_test , train_tokens, test_tokens = vw.convert_tokens_labels_list_to_ids_list(tokens, labels, embedding, max_seq_length, labelclass_to_id, n_tags)\n",
    "    \n",
    "    ##get the specific model\n",
    "    resetSeeds()\n",
    "    model_factory = ModelFactory()\n",
    "    model = model_factory.getModel(extraction_of, max_seq_length, vocab_size, embedding_vectors, n_tags)\n",
    "    \n",
    "    history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=16,\n",
    "    validation_split = 0.2,\n",
    "    verbose = 1,\n",
    "    epochs=epochs_parameter[extraction_of])\n",
    "    \n",
    "    \n",
    "      #to evaluate metrics\n",
    "    performance = PerformanceViewer(labelclass_to_id)\n",
    "    \n",
    "    results.append(performance.classicEvalCrf(model,x_test,y_test, test_tokens))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adequate-plaza",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B_S_pred</th>\n",
       "      <th>I_S_pred</th>\n",
       "      <th>O_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B_S_true</th>\n",
       "      <td>1284.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1156.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I_S_true</th>\n",
       "      <td>84.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>980.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O_true</th>\n",
       "      <td>307.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>33939.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          B_S_pred  I_S_pred   O_pred\n",
       "B_S_true    1284.0       5.0   1156.0\n",
       "I_S_true      84.0      33.0    980.0\n",
       "O_true       307.0      22.0  33939.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.752\n",
      "recall:  0.515\n",
      "f1:  0.612\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B_A_pred</th>\n",
       "      <th>I_A_pred</th>\n",
       "      <th>O_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B_A_true</th>\n",
       "      <td>1007.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1038.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I_A_true</th>\n",
       "      <td>48.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>694.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O_true</th>\n",
       "      <td>604.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>33885.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          B_A_pred  I_A_pred   O_pred\n",
       "B_A_true    1007.0      28.0   1038.0\n",
       "I_A_true      48.0     299.0    694.0\n",
       "O_true       604.0     305.0  33885.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.677\n",
      "recall:  0.582\n",
      "f1:  0.626\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BM_pred</th>\n",
       "      <th>IM_pred</th>\n",
       "      <th>O_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BM_true</th>\n",
       "      <td>382.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>541.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IM_true</th>\n",
       "      <td>14.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O_true</th>\n",
       "      <td>151.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>36838.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         BM_pred  IM_pred   O_pred\n",
       "BM_true    382.0      0.0    541.0\n",
       "IM_true     14.0     27.0    193.0\n",
       "O_true     151.0      8.0  36838.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.817\n",
      "recall:  0.508\n",
      "f1:  0.627\n",
      "overall f1-score  0.621\n"
     ]
    }
   ],
   "source": [
    "# display the results\n",
    "overall_f1 = 0\n",
    "for result in results:\n",
    "    display(result[0])\n",
    "    print(\"precision: \",\"{:.3f}\".format(result[1]))\n",
    "    print(\"recall: \",\"{:.3f}\".format(result[2]))\n",
    "    print(\"f1: \",\"{:.3f}\".format(result[3]))\n",
    "    overall_f1 += result[3]\n",
    "    \n",
    "print(\"overall f1-score \", \"{:.3f}\".format((overall_f1)/3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
